{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Zinc Tutorial 1 - 200 Dalton Dataset"]},{"cell_type":"markdown","metadata":{"id":"u5fN-FXIBlJT"},"source":["## Preliminaries:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28654,"status":"ok","timestamp":1677772386054,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"wKilBa9_2cEm","outputId":"9c074e63-3033-428e-c7f1-c36f4ffbe456"},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import torch.nn.functional as F\n","\n","# Colab - Pytorch Geometric installation according to Pytorch documentation\n","\n","#os.environ['TORCH'] = torch.__version__\n","#print(torch.__version__)\n","#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","\n","\n","from torch_geometric.data import Dataset, Data\n","from torch_geometric.loader import DataLoader\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv, global_mean_pool\n","\n","from IPython.display import Javascript"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3372,"status":"ok","timestamp":1677772389419,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"s4FEnh3l2cEr","outputId":"0db23059-888a-444f-9db9-879dbf478540"},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\yuval\\Projects\\MolecularGraphs\n"]}],"source":["# Local machine\n","%cd C:\\Users\\yuval\\Projects\\MolecularGraphs\n","\n","# Colab\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#%cd drive/MyDrive/MolecularGraphs/\n"]},{"cell_type":"markdown","metadata":{"id":"96yQBi8V2cEr"},"source":["## Preparing the dataset:"]},{"cell_type":"markdown","metadata":{"id":"eM3Cl8cj4Teb"},"source":["### Defining custom Dataset:"]},{"cell_type":"markdown","metadata":{"id":"ha2Nnp9x2cEt"},"source":["The `Dataset` class is designed to sample batches from storage without uploading all data into the RAM.\n","To create our own custom operation we need to create a class that inherent from `Dataset` class.\n","\n","In the `init` method, the arguments that are pass to `Dataset` are:\n","* `root` (str, optional) - The root directory where the data should be saved.\n","This directory is going to have `raw` directory and `processed` directory.\n","The `raw` directory is where you have all files of the data, a file per instance.\n","The `processed` directory is where the class is going to saved all processed files.\n","The processing of files in our case is the convertion of the file into a `Data` object (including node features, edge index, label/s, and optional of edges features).\n","* `transform` (callable, optional) - not used - a function/transform that takes in an `Data` object and returns a transformed version. The `Data` object will be **transformed before every access**.\n","* `pre_transform` (callable, optional) - not used â€“ a function/transform that takes in an `Data` object and returns a transformed version. The `Data` object will be **transformed before being saved to disk**. (default: None)\n","* `pre_filter` (callable, optional) - not used - a function that takes in an `Data` object and returns a boolean value, indicating whether the `Data` object should be included in the final dataset. \n","* `log` (bool, optional) - whether to print any console output while downloading and processing the dataset.\n","\n","Following the `init` method, we have two method decorated as property.\n","The decorator define the method as a \"getter\", i.e., getting an attribute of the class.\n","That means we can treat such method as an attribute and call it without parentheses.\n","Those two properties return all files names inside the previously mentioned directories - `raw` and `processed`.\n","Those two properties are designed for the class to check if the raw/processes files exsit before performing the dolwnloading/processing.\n","\n","The `process` method is called with calling the `Dataset`'s `init` method (I think).\n","In this method you iterate over all of the raw files and turn them into `Data` object of graph, including the `pre_transform` and `pre_filter` functions calls.\n","\n","The `len` and `get` are self-explanable.\n"]},{"cell_type":"markdown","metadata":{"id":"BKez0sWtXP5T"},"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677772389420,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"p8aXFs3d2cEv"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n","        super().__init__(root, transform, pre_transform, pre_filter)\n","\n","    @property\n","    def raw_file_names(self):\n","        return os.listdir(self.raw_dir)\n","\n","    @property\n","    def processed_file_names(self):\n","        data_file_names = [os.path.splitext(file_name)[0]+ '.pt' for file_name in self.raw_file_names]\n","        return data_file_names + ['pre_filter.pt', 'pre_transform.pt']\n","\n","    @property\n","    def num_classes(self):\n","      return 2\n","\n","    def process(self):\n","        idx = 0\n","        for raw_path in self.raw_paths:\n","\n","            # Load the two arrays and scaler from the saved file using read_pickle()\n","            with open(raw_path, 'rb') as f:\n","                x, edge_index, y = pd.read_pickle(f)\n","\n","            data_i = Data(x=torch.tensor(x, dtype=torch.float), \n","                          edge_index=torch.tensor(edge_index, dtype=torch.long),\n","                          y=torch.tensor([int(y)], dtype=torch.long)) # You want the y as 1D int and not a scaler\n","\n","            if self.pre_filter is not None and not self.pre_filter(data_i):\n","                continue\n","\n","            if self.pre_transform is not None:\n","                data_i = self.pre_transform(data_i)\n","\n","            torch.save(data_i, os.path.join(self.processed_dir, f'data_{idx}.pt'))\n","            idx += 1\n","\n","    def len(self):\n","        return len(self.processed_file_names) - 2 # minus the pre_filter and pre_transform\n","\n","    def get(self, idx):\n","        data_i = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n","        return data_i"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1026,"status":"ok","timestamp":1677772390439,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"ycEHMDvE4l0s"},"outputs":[],"source":["dataset = MyDataset('Zinc_200D/GraphData')"]},{"cell_type":"markdown","metadata":{"id":"49rlrZdI4YBn"},"source":["### Examination:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dTGsF5GS2cEw","outputId":"d0fb457a-b896-4c38-da88-1ea8b1b7259d"},"outputs":[{"data":{"text/plain":["Data(x=[19, 11], edge_index=[2, 40], y=[1])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data = dataset.get(10)\n","data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"M6BXVPJM2cEw","outputId":"d0a5c249-9e89-4a92-a1fb-8ca7f667871f"},"outputs":[{"data":{"text/plain":["tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data.x"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dm8LEbPX2cEx","outputId":"faff1f32-35de-4434-ac38-e9728fec38e6"},"outputs":[{"data":{"text/plain":["tensor([[ 0,  1,  1,  1,  2,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,  7,  7,  8,\n","          8,  9,  9,  9, 10, 11, 11, 12, 12, 13, 13, 13, 14, 14, 15, 15, 16, 16,\n","         17, 17, 18, 18],\n","        [ 1,  0,  2,  3,  1,  1,  4,  3,  5,  4,  6, 13,  5,  7, 12,  6,  8,  7,\n","          9,  8, 10, 11,  9,  9, 12,  6, 11,  5, 14, 18, 13, 15, 14, 16, 15, 17,\n","         16, 18, 13, 17]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data.edge_index"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"8HvdEyhnXP5Y","outputId":"ec161b9a-8e05-43cf-db08-35facc34f07b"},"outputs":[{"data":{"text/plain":["tensor([0])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data.y"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Y1dIIKI12cEy"},"outputs":[],"source":["loader = DataLoader(dataset, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["data = next(iter(loader))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["torch_geometric.data.batch.DataBatch"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["type(data)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","32\n","24\n"]}],"source":["for data in loader:\n","    print(len(data))"]},{"cell_type":"markdown","metadata":{"id":"0gJ-RKAY2cEy"},"source":["To split the data into training, validation and test sets, we use the `index_select` method which creates a subset of the dataset from specified indices idx."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6wg07Oe2cEy"},"outputs":[],"source":["N = dataset.len()\n","idx = torch.randperm(N) # Random permutation of integers from 0 to N - 1\n","idx_train, idx_val, idx_test = idx[:int(0.8 * N)], idx[int(0.8 * N): int(0.9 * N)], idx[int(0.9 * N):]\n","\n","train_dataset = dataset.index_select(idx_train)\n","val_dataset = dataset.index_select(idx_val)\n","test_dataset = dataset.index_select(idx_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffXhq27g2cEy"},"outputs":[],"source":["dataset.len()"]},{"cell_type":"markdown","metadata":{"id":"nQTGRXOS2cEz"},"source":["Now we define the `Dataloader`.\n","Note for thet `shuffle` parameter, if set to True, the data will be reshuffled at every epoch.\n","We do not want such thing for the validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZoUHxzA2cEz"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRsQl6O02cEz"},"outputs":[],"source":["for step, data in enumerate(train_loader):\n","    print(f'Step {step + 1}:')\n","    print('=======')\n","    print(f'Number of graphs in the current batch: {data.num_graphs}')\n","    print(data)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"aun3SS5p2cEz"},"source":["## Training:"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1677775814454,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"9_1jHMv82cEz"},"outputs":[],"source":["class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels, dropout_p):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.dropout_p = dropout_p\n","        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n","        self.conv2= GCNConv(hidden_channels, hidden_channels)\n","        self.conv3= GCNConv(hidden_channels, hidden_channels)\n","        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n","        self.lin = Linear(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index, batch):\n","        # 1. Obtain node embeddings \n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        x = self.conv3(x, edge_index)\n","        x = x.relu()\n","        x = self.conv4(x, edge_index)\n","\n","        # 2. Readout layer\n","        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n","\n","        # 3. Apply a final classifier\n","        x = F.dropout(x, p=self.dropout_p, training=self.training)\n","        x = self.lin(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":537,"status":"ok","timestamp":1677775816799,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"cCsgC2Jy2cE0"},"outputs":[],"source":["def progress_bar(iteration, total, size=30):\n","    running = iteration < total\n","    c = \">\" if running else \"=\"\n","    p = (size - 1) * iteration // total\n","    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n","    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n","    return fmt.format(*params)\n","\n","def print_status_bar(iteration, total, metrics):\n","    end = \"\" if iteration < total else \"\\n\"\n","    metric_str = \" - \".join([\"{}: {:.4f}\".format(m, metrics[m]) for m in metrics])\n","    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metric_str), end=end)\n","\n","def train(model, criterion, optimizer, loader, device, frac_batches=1):\n","    \n","    model.train()\n","    total_batch_num = int(len(loader) * frac_batches) # total number of batches used in the evaluation\n","    \n","    # running metrics\n","    weighted_loss_sum = 0\n","    total_instances_num = 0\n","\n","    # Iterate in batches over the training dataset. data is a DataBatch object\n","    for current_batch_i, data in enumerate(loader):  \n","        if current_batch_i + 1 > total_batch_num:\n","           break\n","\n","        data = data.to(device)\n","        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n","        loss = criterion(out, data.y)  # Compute the loss.\n","        loss.backward()  # Derive gradients.\n","        optimizer.step()  # Update parameters based on gradients.\n","        optimizer.zero_grad()  # Clear gradients.\n","        \n","        # printing progress\n","        weighted_loss_sum += len(data) * loss\n","        total_instances_num += len(data)\n","        metrics = {'loss': weighted_loss_sum / total_instances_num} # weighted mean with respect to batches\n","        print_status_bar(current_batch_i + 1, total_batch_num, metrics)\n","         \n","\n","@torch.no_grad()\n","def eval(model, criterion, loader, device):\n","    model.eval()\n","\n","    # running metrics\n","    weighted_loss_sum = 0\n","    total_instances_num = 0\n","    correct_sum = 0\n","\n","    # Iterate in batches over the training/test dataset. Each data is \n","    for data in loader:  \n","\n","        data = data.to(device)\n","        out = model(data.x, data.edge_index, data.batch)\n","         \n","        # Compute the loss sum\n","        weighted_loss_sum += len(data) * criterion(out, data.y)  \n","        \n","        # Compute accuracy\n","        pred = out.argmax(dim=1)  # Use the class with highest probability.\n","        correct_sum += int((pred == data.y).sum())  # summing the number of correct instances predictions\n","        total_instances_num += len(data)\n","\n","    acc = correct_sum / total_instances_num  # number of correct instances predictions devided by the number of instances\n","    loss_mean = weighted_loss_sum / total_instances_num # weighted mean with respect to batches\n","    return acc, loss_mean"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":618,"status":"ok","timestamp":1677775881521,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"5FSqyAP1LV4a"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","lr = 0.008\n","batch_size = 32\n","hidden_channels=64\n","dropout_p = 0.15\n","\n","N = dataset.len()\n","idx = torch.randperm(N) # Random permutation of integers from 0 to N - 1\n","idx_train, idx_val, idx_test = idx[:int(0.8 * N)], idx[int(0.8 * N): int(0.9 * N)], idx[int(0.9 * N):]\n","\n","model = GCN(hidden_channels=hidden_channels, dropout_p=dropout_p).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","train_dataset = dataset.index_select(idx_train)\n","val_dataset = dataset.index_select(idx_val)\n","test_dataset = dataset.index_select(idx_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200407,"status":"ok","timestamp":1677776247814,"user":{"displayName":"Yuval Bayer","userId":"06448299427644141054"},"user_tz":-120},"id":"VLtS-5fO8zTE","outputId":"bc6e7542-2bca-43ed-e9f9-a9df42fc5ce2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","2368/2368 [==============================] - loss: 0.2461\n","Train Acc: 28.3243, Train loss: 0.2396\n"," Val Acc: 25.0000 Val loss: 0.2750\n","Epoch: 2\n","2368/2368 [==============================] - loss: 0.2452\n","Train Acc: 28.9595, Train loss: 0.2263\n"," Val Acc: 25.8000 Val loss: 0.2552\n","Epoch: 3\n","2368/2368 [==============================] - loss: 0.2432\n","Train Acc: 28.9324, Train loss: 0.2329\n"," Val Acc: 26.1000 Val loss: 0.2608\n","Epoch: 4\n","2368/2368 [==============================] - loss: 0.2339\n","Train Acc: 28.9054, Train loss: 0.2368\n"," Val Acc: 26.2000 Val loss: 0.2633\n","Epoch: 5\n","2368/2368 [==============================] - loss: 0.2399\n","Train Acc: 29.0270, Train loss: 0.2304\n"," Val Acc: 26.2000 Val loss: 0.2663\n","Epoch: 6\n","2368/2368 [==============================] - loss: 0.2383\n","Train Acc: 29.1757, Train loss: 0.2144\n"," Val Acc: 26.5000 Val loss: 0.2395\n","Epoch: 7\n","2368/2368 [==============================] - loss: 0.2397\n","Train Acc: 29.1351, Train loss: 0.2145\n"," Val Acc: 26.3000 Val loss: 0.2407\n","Epoch: 8\n","2368/2368 [==============================] - loss: 0.2371\n","Train Acc: 29.2568, Train loss: 0.2225\n"," Val Acc: 26.5000 Val loss: 0.2314\n","Epoch: 9\n","2368/2368 [==============================] - loss: 0.2283\n","Train Acc: 28.8108, Train loss: 0.2309\n"," Val Acc: 25.8000 Val loss: 0.2636\n","Epoch: 10\n","2368/2368 [==============================] - loss: 0.2332\n","Train Acc: 29.0000, Train loss: 0.2159\n"," Val Acc: 26.2000 Val loss: 0.2494\n","Epoch: 11\n","2368/2368 [==============================] - loss: 0.2246\n","Train Acc: 29.0811, Train loss: 0.2120\n"," Val Acc: 26.3000 Val loss: 0.2358\n","Epoch: 12\n","2368/2368 [==============================] - loss: 0.2458\n","Train Acc: 28.9595, Train loss: 0.2272\n"," Val Acc: 26.4000 Val loss: 0.2560\n","Epoch: 13\n","2368/2368 [==============================] - loss: 0.2329\n","Train Acc: 28.6351, Train loss: 0.2436\n"," Val Acc: 25.7000 Val loss: 0.2728\n","Epoch: 14\n","2368/2368 [==============================] - loss: 0.2272\n","Train Acc: 28.6757, Train loss: 0.2418\n"," Val Acc: 25.8000 Val loss: 0.2627\n","Epoch: 15\n","2368/2368 [==============================] - loss: 0.2198\n","Train Acc: 28.9459, Train loss: 0.2229\n"," Val Acc: 26.0000 Val loss: 0.2602\n","Epoch: 16\n","2368/2368 [==============================] - loss: 0.2281\n","Train Acc: 29.1486, Train loss: 0.2219\n"," Val Acc: 26.5000 Val loss: 0.2505\n","Epoch: 17\n","2368/2368 [==============================] - loss: 0.2223\n","Train Acc: 28.5946, Train loss: 0.2358\n"," Val Acc: 26.0000 Val loss: 0.2544\n","Epoch: 18\n","2368/2368 [==============================] - loss: 0.2220\n","Train Acc: 29.1892, Train loss: 0.2152\n"," Val Acc: 26.4000 Val loss: 0.2467\n","Epoch: 19\n","2368/2368 [==============================] - loss: 0.2280\n","Train Acc: 28.1757, Train loss: 0.2596\n"," Val Acc: 25.3000 Val loss: 0.2971\n","Epoch: 20\n","2368/2368 [==============================] - loss: 0.2203\n","Train Acc: 28.3514, Train loss: 0.2618\n"," Val Acc: 25.3000 Val loss: 0.2974\n","Epoch: 21\n","2368/2368 [==============================] - loss: 0.2245\n","Train Acc: 29.3108, Train loss: 0.1991\n"," Val Acc: 26.4000 Val loss: 0.2363\n","Epoch: 22\n","2368/2368 [==============================] - loss: 0.2105\n","Train Acc: 28.8514, Train loss: 0.2359\n"," Val Acc: 25.9000 Val loss: 0.2856\n","Epoch: 23\n","2368/2368 [==============================] - loss: 0.2339\n","Train Acc: 29.2838, Train loss: 0.2016\n"," Val Acc: 26.4000 Val loss: 0.2290\n","Epoch: 24\n","2368/2368 [==============================] - loss: 0.2090\n","Train Acc: 29.3784, Train loss: 0.1969\n"," Val Acc: 26.7000 Val loss: 0.2286\n"]}],"source":["training_metrics = []\n","for epoch in range(1, 25):\n","    print(f'Epoch: {epoch}')\n","    train(criterion, optimizer, train_loader, device)\n","\n","    train_acc, train_loss = eval(model, criterion, train_loader, device)\n","    val_acc, val_loss = eval(model, criterion, val_loader, device)\n","    training_metrics.append([train_acc, train_loss, val_acc, val_loss])\n","    print(f'Train Acc: {train_acc:.4f}, Train loss: {train_loss:.4f}\\n Val Acc: {val_acc:.4f}, Val loss: {val_loss:.4f}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["u5fN-FXIBlJT","eM3Cl8cj4Teb"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"pytorch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"789b5c918ca4ed31f62df3a061c2d98786479cc50a729b77b7a1d38d9deddd09"}}},"nbformat":4,"nbformat_minor":0}
