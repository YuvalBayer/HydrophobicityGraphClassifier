{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuval\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuval\\Projects\\MolecularGraphs\n"
     ]
    }
   ],
   "source": [
    "# Local machine\n",
    "%cd C:\\Users\\yuval\\Projects\\MolecularGraphs\n",
    "\n",
    "# Colab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class is designed to sample batches from storage without uploading all data into the RAM.\n",
    "To create our own custom operation we need to create a class that inherent from `Dataset` class.\n",
    "\n",
    "In the `init` method, the arguments that are pass to `Dataset` are:\n",
    "* `root` (str, optional) - The root directory where the data should be saved.\n",
    "This directory is going to have `raw` directory and `processed` directory.\n",
    "The `raw` directory is where you have all files of the data, a file per instance.\n",
    "The `processed` directory is where the class is going to saved all processed files.\n",
    "The processing of files in our case is the convertion of the file into a `Data` object (including node features, edge index, label/s, and optional of edges features).\n",
    "* `transform` (callable, optional) - not used - a function/transform that takes in an `Data` object and returns a transformed version. The `Data` object will be **transformed before every access**.\n",
    "* `pre_transform` (callable, optional) - not used â€“ a function/transform that takes in an `Data` object and returns a transformed version. The `Data` object will be **transformed before being saved to disk**. (default: None)\n",
    "* `pre_filter` (callable, optional) - not used - a function that takes in an `Data` object and returns a boolean value, indicating whether the `Data` object should be included in the final dataset. \n",
    "* `log` (bool, optional) - whether to print any console output while downloading and processing the dataset.\n",
    "\n",
    "Following the `init` method, we have two method decorated as property.\n",
    "The decorator define the method as a \"getter\", i.e., getting an attribute of the class.\n",
    "That means we can treat such method as an attribute and call it without parentheses.\n",
    "Those two properties return all files names inside the previously mentioned directories - `raw` and `processed`.\n",
    "\n",
    "The `process` method is called with calling the `Dataset`'s `init` method (I think).\n",
    "In this method you iterate over all of the raw files and turn them into `Data` object of graph, including the `pre_transform` and `pre_filter` functions calls.\n",
    "\n",
    "The `len` and `get` are self-explanable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return os.listdir(self.raw_dir)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return glob('Zinc\\GraphData\\processed\\data_*.pt') # Avoiding processed filters and transformers\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        for raw_path in self.raw_paths:\n",
    "\n",
    "            # Load the two arrays and scaler from the saved file using read_pickle()\n",
    "            with open(raw_path, 'rb') as f:\n",
    "                x, edge_index, y = pd.read_pickle(f)\n",
    "            \n",
    "            data_i = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data_i):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data_i = self.pre_transform(data_i)\n",
    "\n",
    "            torch.save(data_i, os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "            idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data_i = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset('Zinc\\GraphData')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 11], edge_index=[2, 40], y=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.get(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  1,  1,  2,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,  7,\n",
       "         7,  8,  8,  9,  9,  9, 10, 11, 11, 12, 12, 13, 13, 13, 14, 14,\n",
       "        15, 15, 16, 16, 17, 17, 18, 18],\n",
       "       [ 1,  0,  2,  3,  1,  1,  4,  3,  5,  4,  6, 13,  5,  7, 12,  6,\n",
       "         8,  7,  9,  8, 10, 11,  9,  9, 12,  6, 11,  5, 14, 18, 13, 15,\n",
       "        14, 16, 15, 17, 16, 18, 13, 17]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data into training, validation and test sets, we use the `index_select` method which creates a subset of the dataset from specified indices idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = dataset.len()\n",
    "idx = torch.randperm(N) # Random permutation of integers from 0 to N - 1\n",
    "idx_train, idx_val, idx_test = idx[:int(0.8 * N)], idx[int(0.8 * N): int(0.9 * N)], idx[int(0.9 * N):]\n",
    "\n",
    "train_dataset = dataset.index_select(idx_train)\n",
    "val_dataset = dataset.index_select(idx_val)\n",
    "test_dataset = dataset.index_select(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2936"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.len()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `Dataloader`.\n",
    "Note for thet `shuffle` parameter, if set to True, the data will be reshuffled at every epoch.\n",
    "We do not want such thing for the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1221], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1175], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1130], ptr=[65])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1131], ptr=[65])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1063], ptr=[65])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1241], ptr=[65])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1206], ptr=[65])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1089], ptr=[65])\n",
      "\n",
      "Step 9:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1207], ptr=[65])\n",
      "\n",
      "Step 10:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1345], ptr=[65])\n",
      "\n",
      "Step 11:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1090], ptr=[65])\n",
      "\n",
      "Step 12:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1105], ptr=[65])\n",
      "\n",
      "Step 13:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1277], ptr=[65])\n",
      "\n",
      "Step 14:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1278], ptr=[65])\n",
      "\n",
      "Step 15:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1152], ptr=[65])\n",
      "\n",
      "Step 16:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1199], ptr=[65])\n",
      "\n",
      "Step 17:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1054], ptr=[65])\n",
      "\n",
      "Step 18:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1178], ptr=[65])\n",
      "\n",
      "Step 19:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1168], ptr=[65])\n",
      "\n",
      "Step 20:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1164], ptr=[65])\n",
      "\n",
      "Step 21:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1323], ptr=[65])\n",
      "\n",
      "Step 22:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1138], ptr=[65])\n",
      "\n",
      "Step 23:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1131], ptr=[65])\n",
      "\n",
      "Step 24:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[988], ptr=[65])\n",
      "\n",
      "Step 25:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1279], ptr=[65])\n",
      "\n",
      "Step 26:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1130], ptr=[65])\n",
      "\n",
      "Step 27:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1173], ptr=[65])\n",
      "\n",
      "Step 28:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1239], ptr=[65])\n",
      "\n",
      "Step 29:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1213], ptr=[65])\n",
      "\n",
      "Step 30:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1224], ptr=[65])\n",
      "\n",
      "Step 31:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1204], ptr=[65])\n",
      "\n",
      "Step 32:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1253], ptr=[65])\n",
      "\n",
      "Step 33:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1107], ptr=[65])\n",
      "\n",
      "Step 34:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1079], ptr=[65])\n",
      "\n",
      "Step 35:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1047], ptr=[65])\n",
      "\n",
      "Step 36:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[64], edge_index=[64], y=[64], batch=[1227], ptr=[65])\n",
      "\n",
      "Step 37:\n",
      "=======\n",
      "Number of graphs in the current batch: 44\n",
      "DataBatch(x=[44], edge_index=[44], y=[44], batch=[665], ptr=[45])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "         loss_sum += loss\n",
    "\n",
    "def eval(criterion, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "         \n",
    "        # Compute the loss sum\n",
    "        loss_sum += criterion(out, data.y)  \n",
    "        \n",
    "        # Compute accuracy\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "\n",
    "    acc = correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    loss_mean = loss_sum / len(loader.dataset)\n",
    "    return acc, loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCN(hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = dataset.index_select(idx_train).to(device)\n",
    "val_dataset = dataset.index_select(idx_val).to(device)\n",
    "test_dataset = dataset.index_select(idx_test).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train(criterion, optimizer, train_loader)\n",
    "    train_acc, train_loss = eval(criterion, train_loader)\n",
    "    val_acc, val_loss = eval(criterion, val_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "789b5c918ca4ed31f62df3a061c2d98786479cc50a729b77b7a1d38d9deddd09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
